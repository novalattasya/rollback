---
title: 'Cara Kerja Machine Learning Seperti ChatGPT, Claude, dan Gemini — Dari Dataset Sampai Jadi AI yang Bisa Ngomong, Gambar, dan Nyanyi'
date: 2025-10-28
description: "Penjelasan lengkap dan santai tentang bagaimana AI seperti ChatGPT, Claude, Gemini bisa menghasilkan teks, gambar, dan musik."
tags: ['AI', 'machine-learning', 'tech']
image: './banner.png'
authors: ['nopal']
draft: false
---
import Callout from '@/components/Callout.astro'

# Cara Kerja Machine Learning Seperti ChatGPT, Claude, dan Gemini  

Pernah gak sih kamu mikir, gimana caranya AI kayak ChatGPT dan Claude bisa nyusun kalimat dengan rapi, jawab pertanyaan, bahkan bisa nyeritain kisah yang emosional banget seolah-olah “hidup”?  Atau gimana bisa AI kayak Midjourney dan DALL·E ngebuat gambar cuma dari teks, dan AI kayak Sora dan Veo yang bisa bikin video, atau mungkin Suno dan Udio yang bisa bikin musik yang terdengar *asli banget*?  Jawabannya: semuanya berawal dari **machine learning** — lebih tepatnya **deep learning**.

Yuk, kita bedah satu-satu. Tapi santai aja, aku bakal jelasin kayak ngobrol sama temen nongkrong.

---

## 1. Dasar Konsep Machine Learning

Machine Learning (ML) itu intinya: **mesin belajar dari data**.  
Alih-alih dikasih instruksi satu-satu kayak “kalau A maka B”, ML justru dikasih **banyak contoh** dan disuruh **nemuin polanya sendiri**.

Contoh:
- Kalau kamu kasih ribuan foto kucing dan anjing, AI bakal belajar bedain keduanya.
- Kalau kamu kasih miliaran kalimat, AI bakal belajar gimana manusia nyusun kata.

<Callout variant='note'>
Intinya, ML = <kbd>Data</kbd> + <kbd>Algoritma</kbd> + <kbd>Komputasi</kbd>
</Callout>

| `Komponen` | `Penjelasan` |
|-----------|--------------------|
| <kbd>**Data**</kbd> | Contoh-contoh yang dipelajari mesin (teks, gambar, suara, dll) |
| <kbd>**Model/Arsitektur**</kbd> | “Otak” AI yang memproses data dan belajar pola |
| <kbd>**Training**</kbd> | Proses melatih AI agar ngerti pola dari data |
| <kbd>**Parameter**</kbd> | Angka-angka internal yang AI ubah selama belajar |
| <kbd>**Loss Function**</kbd> | Cara mengukur seberapa “bodoh” AI saat ini |
| <kbd>**Optimizer**</kbd> | Algoritma yang bantu AI jadi “lebih pintar” tiap langkah |

<Callout variant="important">
  Jadi cara kerja Machine Learning bukan diberi perintah kayak gini:
  ```python name="python.py"
  A = False
  C = True
  if A:
    print("B terjadi")        # kalau A benar
    elif C:
    print("D terjadi")        # kalau A salah tapi C benar
    else:
    print("E terjadi")        # kalau dua-duanya salah
</Callout>
---

## 2. Dari Data ke Pengetahuan: Dataset Itu Segalanya

AI gak bisa belajar tanpa **dataset**.  
Dataset adalah bahan mentahnya.

Misal:
- ChatGPT → teks dari buku, web, forum, dan dataset kurasi (Common Crawl, Wikipedia, arXiv)
- Midjourney / DALL·E → gambar dengan caption deskriptif
- Suno / Udio → file audio + lirik + metadata musik

#### Proses Dataset:
1. **Kumpulin data** (bisa ratusan terabyte)
2. **Bersihin data** (hapus spam, iklan, teks rusak)
3. **Normalisasi** (misal ubah semua ke format teks UTF-8)
4. **Tokenisasi** (ubah teks ke angka)

Contoh tokenisasi sederhana:
```bash
Teks: "Hello World!"
Token: ['5021', '7123']

````

---

## 3. Arsitektur Model — Otak AI Itu Jaringan Saraf

AI modern kayak ChatGPT, Claude, dan Gemini pakai **transformer architecture**.  
Ini bukan Optimus Prime, tapi prinsip dasarnya mirip: **semuanya bekerja paralel dan efisien**.

### Gambaran Kasar Arsitektur Transformer

<kbd>Input Teks</kbd> > <kbd>Embedding Layer</kbd> > <kbd>Multi-Head Attention</kbd> > <kbd>Feed Forward Network</kbd> > <kbd>Normalization</kbd> > <kbd>Output Token</kbd>

Setiap “blok” ini bisa terdiri dari **miliaran parameter**, dan diulang ratusan kali.

#### Komponen Utama:

* **Embedding Layer**: ubah kata jadi vektor angka (representasi makna)
* **Self-Attention**: lihat konteks kata lain di kalimat (ini yang bikin AI “ngerti konteks”)
* **Feed Forward**: proses internal non-linear (tempat AI “berpikir”)
* **Layer Normalization**: jaga kestabilan selama training
* **Output Layer**: hasilkan prediksi token berikutnya

---

## 4. Proses Training — “Sekolahnya” AI

Bayangin kamu ngajarin AI kayak ngajarin anak kecil nulis karangan.

1. Kasih contoh kalimat:
   `"Aku suka makan bakso karena..."`
2. AI tebak kata berikutnya: `"pedas"`
3. Cek jawaban, bandingin dengan kebenaran: `"enak"`
4. Hitung **loss** (seberapa jauh tebakan dari kebenaran)
5. Koreksi bobot (parameter) pakai **backpropagation**
6. Ulangi jutaan kali sampai hasilnya bagus

#### Training Loop Sederhana:

```python title="python.py"
for epoch in range(epochs):
    for batch in data:
        y_pred = model(batch)
        loss = criterion(y_pred, y_true)
        optimizer.step(loss)
```

---

## 5. Parameter, Gradient, dan Backpropagation

<kbd>Parameter</kbd> = **ingatan AI**.
Misalnya, ChatGPT punya **ratusan miliar parameter**.

Setiap parameter disesuaikan berdasarkan **gradient descent**.

### Rumus Dasarnya:

$$
\theta_{new} = \theta_{old} - \eta \cdot \frac{\partial L}{\partial \theta}
$$

<Callout variant="definition">
        * ( $\theta$ ): parameter model
        * ( $\eta$ ): learning rate
        * ( $L$ ): loss function
        * ($\frac{\partial L}{\partial \theta}$): turunan gradient
    <Callout variant="tip">
        Artinya: setiap langkah, AI memperbaiki dirinya sedikit demi sedikit supaya lebih tepat.
    </Callout>
</Callout>

---

## 6. Epoch, Step, dan Batch Size

| `Istilah`   | `Arti`                                     | `Analogi`                    |
| --------- | ---------------------------------------- | -------------------------- |
| <kbd>**Epoch**</kbd> | 1 kali seluruh dataset dipelajari        | Satu semester sekolah      |
| <kbd>**Batch**</kbd> | Sekumpulan data kecil dalam satu iterasi | 1 kelas pelajaran          |
| <kbd>**Step**</kbd>  | Satu update parameter                    | 1 kali belajar dalam kelas |

<Callout variant="tip">
    Misal: dataset 1 juta teks, batch size 1000, berarti 1000 step = 1 epoch.
</Callout>
---

## 7. Dari Training ke Inferensi

Setelah model selesai dilatih, kita “bekuin” parameternya.
Lalu saat kamu mengetik di ChatGPT, yang terjadi bukan “belajar ulang”, tapi **inferensi** — alias proses **menggunakan ilmu yang sudah dipelajari**.

### Flow-nya:

<kbd>Input Teks</kbd> → <kbd>Tokenisasi</kbd> → <kbd>Model Transformer</kbd> → <kbd>Detokenisasi</kbd> → <kbd>Output Teks</kbd>

AI prediksi token demi token berdasarkan konteks sebelumnya.

Contoh:

```bash
Input: "Aku lapar, pengen makan"
Prediksi:
→ "bakso"
→ "enak"
→ "banget"
```

![Flow AI memprediksi](https://raw.githubusercontent.com/dair-ai/ml-visuals/refs/heads/master/1.png)

---

## 8. Gimana AI Bisa Bikin Gambar, Musik, dan Video?

Walaupun semua berbasis *machine learning*, tiap domain punya pendekatan beda:

| Jenis AI                | Arsitektur Umum                  | Contoh Model                         |
| ----------------------- | -------------------------------- | ------------------------------------ |
| <kbd>**Teks (NLP)**</kbd>          | Transformer                      | ChatGPT, Claude, Gemini              |
| <kbd>**Gambar**</kbd> | Diffusion, GAN                   | DALL·E, Midjourney, Stable Diffusion |
| <kbd>**Musik/Audio**</kbd>         | Transformer + Spectrogram        | Suno, Udio, MusicLM                  |
| <kbd>**Video**</kbd>               | Diffusion 3D + Temporal Modeling | Sora, Runway, Pika Labs              |

### Contoh cara kerja Diffusion Model (gambar):

1. Ambil gambar dan tambahkan noise → jadi blur
2. AI belajar *menghapus noise* langkah demi langkah
3. Setelah training, AI bisa mulai dari *noise total* → hasil gambar baru

Rumus dasarnya mirip ini:

$$
x_{t-1} = \frac{1}{\sqrt{\alpha_t}} (x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha_t}}} \cdot \epsilon_\theta(x_t, t))
$$

Jangan pusing, intinya AI belajar "ngapus noise" dengan cerdas.

---

## 9. Optimizer, Regularization, dan Fine-Tuning

### Optimizer

Algoritma yang bantu model cepat belajar tanpa nyasar.

Contoh umum:

* **SGD**: sederhana tapi stabil
* **Adam / AdamW**: cepat dan populer
* **RMSProp**: cocok untuk data noisy

### Regularization

Agar AI gak “ngehapal” data (overfitting).
Caranya bisa pakai dropout, weight decay, atau augmentasi data.

### Fine-tuning

Tahap lanjutan: ngelatih ulang model besar biar cocok sama kebutuhan spesifik.

Misal:

* ChatGPT → fine-tuning untuk “suara sopan”
* Gemini → fine-tuning agar bisa multi-modal (teks, gambar, suara)
* Claude → fine-tuning agar punya reasoning lebih kuat

---

## 10. Dari Model Jadi Produk AI

1. **Training di superkomputer (GPU/TPU cluster)**
2. **Evaluasi performa** (pakai dataset validasi)
3. **Deployment ke server**
4. **Optimasi latency & cost**
5. **Tambah guardrail & moderation**

Hasil akhirnya kayak:

* ChatGPT → text interface
* Gemini → AI multimodal
* Claude → AI dengan reasoning kompleks

---

## 11. Kenapa AI Bisa "Tampak Hidup"?

Karena dia **belajar dari pola manusia** — cara menulis, bercanda, marah, sedih.
Tapi dia **nggak punya kesadaran**.
Dia cuma menebak kata berikutnya berdasarkan konteks yang masuk akal.

<Callout variant="note">
    AI bukan manusia, tapi cerminan data manusianya.
</Callout>
---

## 12. Ringkasan Alur Lengkap

<kbd>Dataset Mentah</kbd> → <kbd>Pembersihan Data</kbd> → <kbd>Tokenisasi</kbd> → <kbd>Training Transformer</kbd> → <kbd>Optimisasi Parameter</kbd> → <kbd>Evaluasi</kbd> → <kbd>Deployment Model</kbd> → <kbd>Inferensi User</kbd>

---

## 13. Kesimpulan

Machine learning bukan sihir.
Ia hanyalah matematika, statistik, dan data — tapi diproses dengan skala **gila-gilaan**.

Dengan jutaan GPU, terabyte data, dan ratusan miliar parameter, akhirnya lahirlah AI seperti ChatGPT, Claude, Gemini, Veo, Midjourney, dan lainnya — yang bisa:

* ngobrol kayak manusia,
* bikin gambar kayak seniman,
* dan nyanyi kayak penyanyi profesional.

<Callout variant="important">
    Tapi ingat, semua itu bukan keajaiban… melainkan hasil dari **data, arsitektur, dan proses belajar yang panjang.**
</Callout>

---