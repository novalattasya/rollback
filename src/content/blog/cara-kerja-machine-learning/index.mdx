---
title: 'Cara Kerja Machine Learning Seperti ChatGPT, Claude, dan Gemini — Dari Dataset Sampai Jadi AI yang Bisa Ngomong, Gambar, dan Nyanyi'
date: 2025-10-28
description: "Penjelasan lengkap dan santai tentang bagaimana AI seperti ChatGPT, Claude, Gemini bisa menghasilkan teks, gambar, dan musik."
tags: ['AI', 'machine-learning', 'tech']
image: './banner.png'
authors: ['nopal']
draft: false
---
import Callout from '@/components/Callout.astro'

# Cara Kerja Machine Learning Seperti ChatGPT, Claude, dan Gemini  

Pernah gak sih kamu mikir, gimana caranya AI kayak ChatGPT dan Claude bisa nyusun kalimat dengan rapi, jawab pertanyaan, bahkan bisa nyeritain kisah yang emosional banget seolah-olah seperti makhluk “hidup”?  Atau gimana bisa AI kayak Midjourney dan DALL·E ngebuat gambar cuma dari teks, dan AI kayak Sora dan Veo yang bisa bikin video, atau mungkin Suno dan Udio yang bisa bikin musik yang terdengar *asli banget*?  Jawabannya: semuanya berawal dari **machine learning** — lebih tepatnya **deep learning**.

Kali ini aku akan bedah satu-satu bagaimana teknisnya AI ini bekerja secara detail. Tapi santai aja, aku bakal jelasin secara gampang dan general.

---

## 1. Konsep Dasar Machine Learning

Machine Learning atau kita singkat aja jadi ML, itu intinya: **mesin yang belajar dari data**.  
Alih-alih dikasih instruksi satu-satu kayak “kalau permintaannya A maka jawabannya B”, ML justru dikasih **banyak contoh** dan disuruh **nemuin polanya sendiri**.

Contoh:
- Kalau kamu kasih ribuan foto kucing dan anjing, AI bakal belajar bedain keduanya.
- Kalau kamu kasih miliaran kalimat, AI bakal belajar gimana manusia nyusun kata.

<Callout variant='note'>
Intinya, ML = <kbd>Data</kbd> + <kbd>Algoritma</kbd> + <kbd>Komputasi</kbd>
</Callout>

| Komponen                        | Penjelasan                                                                       |
| ------------------------------- | -------------------------------------------------------------------------------- |
| <kbd>**Data**</kbd>             | Gambar, teks, video, dan audio.                                                  |
| <kbd>**Model Arsitektur**</kbd> | Otak AI yang memproses data dan belajar pola dari data                           |
| <kbd>**Training**</kbd>         | Proses melatih AI agar ngerti pola dari data                                     |
| <kbd>**Parameter**</kbd>        | Angka-angka internal yang AI ubah selama belajar sekaligus menjadi "ingatan"-nya |
| <kbd>**Loss Function**</kbd>    | Cara mengukur seberapa “bodoh” AI saat ini                                       |
| <kbd>**Optimizer**</kbd>        | Algoritma yang bantu AI jadi “lebih pintar” di tiap langkahnya                   |

<Callout variant='important'>
Jadi cara kerja Machine Learning bukan diberi perintah kayak gini:
```python del={1-8}
A = False
C = True
if A:
    print("B terjadi")        # kalau A benar
elif C:
    print("D terjadi")        # kalau A salah tapi C benar
else:
    print("E terjadi")        # kalau dua-duanya salah
```
Cara kerja ML lebih gila dan lebih rumit daripada itu.
</Callout>
---

## 2. AI Mengubah Dataset Mejadi Pengetahuan

*"AI gak bisa belajar tanpa **dataset**".* Dataset adalah dasar dari pengetahuan AI.

Misal:
- ChatGPT / Claude mengambil teks dari buku, web, forum, internet, dan dataset kurasi (Common Crawl, Wikipedia, arXiv)
- Midjourney / DALL·E mengambil ribuan bahkan jutaan gambar dari internet
- Suno / Udio mengambil file audio + lirik + metadata musik

#### Proses Dataset:
1. **Kumpulin data sebanyak mungkin**: _Ukurannya bisa tembus ratusan terabyte_
2. **Bersihin data**: _Hapus spam, iklan, teks rusak, pornografi, kekerasan, pelecehan_
3. **Normalisasi**" _Misal ubah semua ke format teks UTF-8_
4. **Tokenisasi**: _Ubah teks ke angka yang mudah diingat atau diproses AI_

###### Q&A
1. **Kenapa data harus banyak?**  _Karena semakin besar dataset, semakin pintar AI_
2. **Kenapa data harus dibersihkan?** _Karena sebagian besar dataset diambil dari internet—bahkan mungkin aja ambil data dari Darkweb. Jadi bisa saja ada data berbahaya atau hoax yang secara tidak sengaja diambil. Bayangin kalau AI belajar dari data berbahaya itu, mungkin ChatGPT bisa tiba-tiba kirim: "Berikut alat dan bahan merakit b*m panci"_
3. **Kenapa dataset harus di-normalisasi?** _Biar datanya memiliki format yang sama. Tujuannya agar AI gak bingung saat pelatihan_
4. **Kenapa data harus di-tokenisasi?** _Karena itu diperlukan biar proses training AI bisa efisien dan gak boros sumberdaya komputasi_

Contoh tokenisasi sederhana:
```bash {1-2} add={4-5}
text: "Selamat Pagi Semuanya!"
token: ['5021', '7123']

text: "Hello World"
token: ['82']
````

Jadi tokenisasi itu secara kasar adalah supaya saat berpikir, AI gak akan kewalahan untuk memproses data. Alih-alih memikirkan kalimat "Hello World", AI akan memikirkan 82 sebagai gantinya. 

Jadi ketika AI berpikir tentang kalimat _"Selamat Pagi Semuanya! Hello World"_ maka berkat tokenisasi, AI cuman akan berpikir ['5021', 7123', '82].

Lihat kan bedanya? Dari yang awalnya 5 kata, berubah jadi 3 blok angka. Itu efisien banget.

---

## 3. Arsitektur Model — Otak AI Itu Jaringan Saraf
![Saraf AI](https://www.nicepng.com/png/full/880-8805864_neural-networks-are-a-set-of-algorithms-which.png)

AI modern kayak ChatGPT, Claude, dan Gemini pakai **transformer architecture**.  
Ini maksudnya bukan Optimus Prime, tapi prinsip dasarnya mirip: **semuanya bekerja paralel dan efisien**.

### Gambaran Kasar Arsitektur Transformer

<kbd>Input Teks</kbd> → <kbd>Embedding Layer</kbd> → <kbd>Multi-Head Attention</kbd> → <kbd>Feed Forward Network</kbd> → <kbd>Normalization</kbd> → <kbd>Output Token</kbd>

Setiap “blok” ini bisa terdiri dari **miliaran parameter**, dan diulang ratusan kali.

#### Komponen Utama:

* **Embedding Layer**: ubah kata jadi vektor angka *(representasi makna)*
* **Self-Attention**: lihat konteks kata lain di kalimat *(ini yang bikin AI ngerti konteks)*
* **Feed Forward**: proses internal non-linear *(tempat AI berpikir)*
* **Layer Normalization**: jaga kestabilan selama training
* **Output Layer**: hasilkan prediksi token berikutnya

Pusing ya? Jangan dipikirin. Cukup diketahui aja, soalnya ini masalah teknis mendalam yang cuman dipikirkan oleh engineer. Kalo tertarik mau bikin AI boleh aja dipelajari lebih dalam lagi.

---

## 4. Proses Training — Proses melatih AI

Bayangin kamu ngajarin AI kayak ngajarin anak kecil nulis karangan.

1. Kamu kasih contoh kalimat pertanyaan:
   `"Orang Indonesia suka makan bakso karena...?"`
2. AI tebak kata berikutnya dengan membandingkannya dengan dataset lain. Misal dalam dataset ada kalimat:
```mdx showLineNumbers=false
Orang indonesia sangat suka makanan pedas
Makanan Indonesia semuanya enak, terutama bakso
```
3. Maka AI akan melihat pola kalimat dari pertanyaan <kbd>1</kbd> berdasarkan semua kalimat yang ada di dataset, kemudian memberikan tebakan jawaban misalnya: `"Orang Indonesia suka makan bakso karena pedas"`  
4. AI akan cek jawaban, bandingin dengan kalimat yang benar: `"Orang Indonesia suka makan bakso karena enak"`
4. Algoritma akan hitung **loss** / seberapa jauh tebakan dari kebenaran
5. Algoritma kemudian koreksi **parameter** (bobot/ingatan) pakai **backpropagation** untuk simpan tebakan yang benar, yaitu: `"Orang Indonesia suka makan bakso karena enak"`
6. Ulangi jutaan kali sampai hasilnya bagus

<Callout variant="note">
AI akan belajar (Training) sendiri dari dataset. AI akan secara mandiri belajar pola, ngecek jawaban, hitung loss, koreksi parameter, dll. Jadi kalo kamu ngelatih AI, kamu tinggal duduk manis nunggu AI selesai belajar. Analoginya mirip kayak guru sekolah yang cuman memberikan buku paket (dataset) dan nyuruh murid belajar sendiri. Kalo murid udah selesai belajar (Training), guru tinggal memberikan tugas.
</Callout>

#### Contoh Dataset

<iframe
  src="https://huggingface.co/datasets/lv2/Indonesia_LLama/embed/viewer/default/train"
  frameborder="0"
  width="100%"
  height="560px"
></iframe>

*Di atas adalah dataset yang aku buat untuk melatih AI LLama (LLM). Aku sudah membagikannya di [HuggingFace](https://huggingface.co/datasets/lv2/Indonesia_LLama), kamu bisa lihat atau download untuk dipelajari lebih lanjut.*

#### Contoh Training Loop Sederhana:

```python title="python,py" add={1} {2-5}
for epoch in range(epochs):
    for batch in data:
        y_pred = model(batch)
        loss = criterion(y_pred, y_true)
        optimizer.step(loss)
```

Jadi yang di atas itu contoh codingan buat ngelatih AI dari dataset yang sudah disiapkan. Jadi ngelatih AI khususnya LLM, bukan kayak ngajarin anak kecil belajar membaca beneran, tapi memberi perintah pada model AI buat belajar sendiri dari dataset yang diberikan.

---

## 5. Parameter, Gradient, dan Backpropagation

<kbd>Parameter</kbd> = **ingatan AI**.
Misalnya, ChatGPT punya **ratusan miliar parameter**.

Setiap parameter disesuaikan berdasarkan **gradient descent**.

### Rumus Dasarnya:

$$
\theta_{new} = \theta_{old} - \eta \cdot \frac{\partial L}{\partial \theta}
$$

<Callout variant="definition">
        * ( $\theta$ ): parameter model
        * ( $\eta$ ): learning rate
        * ( $L$ ): loss function
        * ($\frac{\partial L}{\partial \theta}$): turunan gradient
    <Callout variant="tip">
        Artinya: setiap langkah, AI memperbaiki dirinya sedikit demi sedikit supaya lebih tepat.
    </Callout>
</Callout>

---

## 6. Epoch, Step, dan Batch Size

| `Istilah`            | `Arti`                                   | `Analogi sekolah`         |
| -------------------- | ---------------------------------------- | ------------------------- |
| <kbd>**Epoch**</kbd> | 1 kali seluruh dataset dipelajari        | Belajar satu semester     |
| <kbd>**Batch**</kbd> | Sekumpulan data kecil dalam satu iterasi | 1 hari belajar di sekolah |
| <kbd>**Step**</kbd>  | Satu update parameter                    | 1 jam mata pelajaran      |

<Callout variant="tip">
    Misal: dataset-nya ada 1 juta teks terus batch sizenya dihitung 1000, berarti 1000 step = 1 epoch.
</Callout>
---

## 7. Dari Training ke Inferensi

Setelah model selesai dilatih, kita “bekuin” parameternya.
Lalu saat kamu mengetik di ChatGPT, yang terjadi bukan “belajar ulang”, tapi **inferensi** — alias proses **menggunakan ilmu yang sudah dipelajari**.

### Flow-nya:

<kbd>Input Teks</kbd> → <kbd>Tokenisasi</kbd> → <kbd>Model Transformer</kbd> → <kbd>Detokenisasi</kbd> → <kbd>Output Teks</kbd>

AI prediksi token demi token berdasarkan konteks sebelumnya.

Contoh:

```bash
Input: "Aku lapar, pengen makan"
Prediksi:
→ "bakso"
→ "enak"
→ "banget"
```

![Transformer](https://cdn.sanity.io/images/kuana2sp/production-main/8f5a2c61102f08343929746d937bfc30cb4c9b3f-4248x2264.png?w=1920&fit=max&auto=format)

---

## 8. Gimana AI Bisa Bikin Gambar, Musik, dan Video?

Walaupun semua berbasis *machine learning*, tiap domain punya pendekatan beda. Jadi cara melatih AI yang bisa bikin gambar, video, dan musik itu berbeda dengan melatih AI berbasis teks (LLM) kayak ChatGPT, Claude, dan Gemini. Tapi meski begitu, intinya sama: `Berikan dataset, dan AI akan belajar sendiri.`

| Jenis AI                | Arsitektur Umum                  | Contoh Model                         |
| ----------------------- | -------------------------------- | ------------------------------------ |
| <kbd>**Teks (NLP)**</kbd>          | Transformer                      | ChatGPT, Claude, Gemini              |
| <kbd>**Gambar**</kbd> | Diffusion, GAN                   | DALL·E, Midjourney, Stable Diffusion |
| <kbd>**Musik/Audio**</kbd>         | Transformer + Spectrogram        | Suno, Udio, MusicLM                  |
| <kbd>**Video**</kbd>               | Diffusion 3D + Temporal Modeling | Sora, Runway, Pika Labs              |

### Contoh cara kerja Diffusion Model (gambar):

1. Ambil gambar dan tambahkan noise → jadi blur
2. AI belajar *menghapus noise* langkah demi langkah
3. Setelah training, AI bisa mulai dari *noise total* → hasil gambar baru

Rumus dasarnya mirip ini:

$$
x_{t-1} = \frac{1}{\sqrt{\alpha_t}} (x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha_t}}} \cdot \epsilon_\theta(x_t, t))
$$

Jangan pusing, intinya AI belajar "ngapus noise" dengan cerdas.

#### Contoh Dataset Untuk Melatih AI Generate Gambar



---

## 9. Optimizer, Regularization, dan Fine-Tuning

### Optimizer

Algoritma yang bantu model cepat belajar tanpa nyasar.

Contoh umum:

* **SGD**: sederhana tapi stabil
* **Adam / AdamW**: cepat dan populer
* **RMSProp**: cocok untuk data noisy

### Regularization

Agar AI gak “ngehapal” data (overfitting).
Caranya bisa pakai dropout, weight decay, atau augmentasi data.

### Fine-tuning

Tahap lanjutan: ngelatih ulang model besar biar cocok sama kebutuhan spesifik.

Misal:

* ChatGPT → fine-tuning untuk “suara sopan”
* Gemini → fine-tuning agar bisa multi-modal (teks, gambar, suara)
* Claude → fine-tuning agar punya reasoning lebih kuat

---

## 10. Dari Model Jadi Produk AI

1. **Training di superkomputer (GPU/TPU cluster)**
2. **Evaluasi performa** (pakai dataset validasi)
3. **Deployment ke server**
4. **Optimasi latency & cost**
5. **Tambah guardrail & moderation**

Hasil akhirnya kayak:

* ChatGPT → text interface
* Gemini → AI multimodal
* Claude → AI dengan reasoning kompleks

---

## 11. Kenapa AI Bisa "Tampak Hidup"?

Karena dia **belajar dari pola manusia** — cara menulis, bercanda, marah, sedih.
Tapi dia **nggak punya kesadaran**.
Dia cuma menebak kata berikutnya berdasarkan konteks yang masuk akal.

<Callout variant="note">
    AI bukan manusia, tapi cerminan data manusianya.
</Callout>
---

## 12. Ringkasan Alur Lengkap

<kbd>Dataset Mentah</kbd> → <kbd>Pembersihan Data</kbd> → <kbd>Tokenisasi</kbd> → <kbd>Training Transformer</kbd> → <kbd>Optimisasi Parameter</kbd> → <kbd>Evaluasi</kbd> → <kbd>Deployment Model</kbd> → <kbd>Inferensi User</kbd>

---

## 13. Kesimpulan

Machine learning bukan sihir.
Ia hanyalah matematika, statistik, dan data — tapi diproses dengan skala **gila-gilaan**.

Dengan jutaan GPU, terabyte data, dan ratusan miliar parameter, akhirnya lahirlah AI seperti ChatGPT, Claude, Gemini, Veo, Midjourney, dan lainnya — yang bisa:

* ngobrol kayak manusia,
* bikin gambar kayak seniman,
* dan nyanyi kayak penyanyi profesional.

<Callout variant="important">
    Tapi ingat, semua itu bukan keajaiban… melainkan hasil dari **data, arsitektur, dan proses belajar yang panjang.**
</Callout>

---